#  Neural Networks - Implementa√ß√µes Educacionais

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![NumPy](https://img.shields.io/badge/NumPy-Latest-orange.svg)](https://numpy.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Educational](https://img.shields.io/badge/Purpose-Educational-purple.svg)](#)

Reposit√≥rio com implementa√ß√µes educacionais de redes neurais em Python puro, focado no aprendizado dos conceitos fundamentais de Machine Learning e Deep Learning.

##  Objetivo

Este reposit√≥rio foi criado para estudantes e entusiastas que desejam entender **como as redes neurais funcionam internamente**, implementando os algoritmos do zero usando apenas NumPy.

##  Estrutura do Projeto

```
Neural_Networks/
‚îú‚îÄ‚îÄ  mlp_simple.py              # Implementa√ß√£o base do MLP
‚îú‚îÄ‚îÄ  exemplos_mlp.py            # Exemplos pr√°ticos e casos de uso
‚îú‚îÄ‚îÄ  funcoes_ativacao.py        # Diferentes fun√ß√µes de ativa√ß√£o
‚îú‚îÄ‚îÄ  requirements.txt           # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ  .gitignore                # Arquivos ignorados pelo Git
‚îî‚îÄ‚îÄ  README.md                 # Este arquivo
```

##  Implementa√ß√µes Dispon√≠veis

### 1. **MLP B√°sico** (`mlp_simple.py`)
-  Forward propagation
-  Backpropagation
-  Gradient descent
-  Fun√ß√£o de ativa√ß√£o Sigmoid
-  Classifica√ß√£o e regress√£o

### 2. **Fun√ß√µes de Ativa√ß√£o** (`funcoes_ativacao.py`)
-  Sigmoid
-  Tanh
-  ReLU
-  Leaky ReLU
-  ELU
-  Swish

### 3. **Exemplos Pr√°ticos** (`exemplos_mlp.py`)
-  Problema XOR
-  Classifica√ß√£o linear
-  C√≠rculos conc√™ntricos
-  Regress√£o n√£o-linear
-  Compara√ß√£o de arquiteturas

## Como Usar

### 1. Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### 2. Executar o Exemplo B√°sico (XOR)

```bash
python mlp_simple.py
```

### 3. Executar Exemplos Avan√ßados

```bash
python exemplos_mlp.py
```

## Conceitos Implementados

### Arquitetura da Rede
- **Camadas totalmente conectadas**: Cada neur√¥nio se conecta a todos os neur√¥nios da pr√≥xima camada
- **Fun√ß√£o de ativa√ß√£o Sigmoid**: Usada em todas as camadas
- **Inicializa√ß√£o Xavier/Glorot**: Para melhor converg√™ncia

### Algoritmos
- **Forward Pass**: Propaga√ß√£o dos dados da entrada at√© a sa√≠da
- **Backpropagation**: C√°lculo dos gradientes e propaga√ß√£o do erro
- **Gradient Descent**: Atualiza√ß√£o dos pesos baseada nos gradientes

### Funcionalidades
-  Classifica√ß√£o bin√°ria
-  Regress√£o
-  M√∫ltiplas camadas ocultas
-  Visualiza√ß√£o de resultados
-  M√©tricas de avalia√ß√£o

## Exemplos Inclu√≠dos

### 1. Problema XOR (B√°sico)
```python
from mlp_simple import MLPSimples
import numpy as np

# Dados XOR
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Criar e treinar rede
mlp = MLPSimples(camadas=[2, 4, 1], taxa_aprendizado=0.5)
mlp.treinar(X, y, epocas=2000)

# Fazer previs√µes
previsoes = mlp.prever(X)
print(previsoes)
```

### 2. Classifica√ß√£o Linear
- Dados sint√©ticos com 2 features
- Separa√ß√£o linear de classes
- Avalia√ß√£o com conjunto de teste

### 3. C√≠rculos Conc√™ntricos (N√£o-linear)
- Problema mais complexo que requer m√∫ltiplas camadas
- Visualiza√ß√£o da fronteira de decis√£o
- Demonstra a capacidade de aprender padr√µes n√£o-lineares

### 4. Regress√£o (Fun√ß√£o Seno)
- Aproxima√ß√£o de fun√ß√£o cont√≠nua
- Demonstra uso da rede para regress√£o
- Visualiza√ß√£o da curva aprendida

### 5. Compara√ß√£o de Arquiteturas
- Testa diferentes n√∫meros de camadas e neur√¥nios
- Compara performance no problema XOR
- Ajuda a entender o impacto da arquitetura

## Par√¢metros da Rede

### Construtor da MLPSimples
```python
MLPSimples(camadas, taxa_aprendizado=0.01)
```

- **`camadas`**: Lista com n√∫mero de neur√¥nios por camada
  - Exemplo: `[2, 4, 3, 1]` = 2 entradas, 4 neur√¥nios na 1¬™ camada oculta, 3 na 2¬™, 1 sa√≠da
- **`taxa_aprendizado`**: Controla o tamanho dos passos na atualiza√ß√£o dos pesos
  - Valores t√≠picos: 0.001 a 1.0
  - Maior = aprendizado mais r√°pido, mas pode ser inst√°vel
  - Menor = aprendizado mais est√°vel, mas mais lento

### M√©todo de Treinamento
```python
treinar(X, y, epocas=1000, verbose=True)
```

- **`X`**: Dados de entrada (matriz numpy)
- **`y`**: R√≥tulos/targets (matriz numpy)
- **`epocas`**: N√∫mero de itera√ß√µes de treinamento
- **`verbose`**: Se deve imprimir progresso

## Interpretando os Resultados

### Curva de Aprendizado
- **Decrescente**: Rede est√° aprendendo
- **Est√°vel**: Convergiu ou precisa de mais √©pocas
- **Oscilante**: Taxa de aprendizado pode estar muito alta

### M√©tricas
- **Acur√°cia**: % de previs√µes corretas (classifica√ß√£o)
- **MSE**: Erro m√©dio quadr√°tico (regress√£o)
- **Diferen√ßa treino/teste**: Indica overfitting se muito grande

## Dicas para Experimentar

### Ajuste de Hiperpar√¢metros
1. **Taxa de Aprendizado**:
   - Comece com 0.1
   - Se n√£o converge: diminua (0.01, 0.001)
   - Se muito lento: aumente (0.5, 1.0)

2. **Arquitetura**:
   - Problemas simples: 1-2 camadas ocultas
   - Problemas complexos: 3-4 camadas ocultas
   - Mais neur√¥nios = mais capacidade, mas risco de overfitting

3. **√âpocas**:
   - Monitore a curva de erro
   - Pare quando estabilizar
   - T√≠pico: 1000-5000 √©pocas

### Problemas Comuns
- **N√£o converge**: Taxa de aprendizado muito alta ou dados n√£o normalizados
- **Converge muito lento**: Taxa de aprendizado muito baixa
- **Overfitting**: Rede muito grande para os dados dispon√≠veis
- **Underfitting**: Rede muito simples para o problema

## Limita√ß√µes desta Implementa√ß√£o

- Apenas fun√ß√£o de ativa√ß√£o Sigmoid
- N√£o inclui regulariza√ß√£o (dropout, L1/L2)
- N√£o otimizada para grandes datasets
- Apenas gradient descent b√°sico (sem momentum, Adam, etc.)
- Sem GPU acceleration

## üõ£Ô∏è Roadmap Futuro

### Pr√≥ximas Implementa√ß√µes
- [ ] **Visualiza√ß√µes**: Fronteiras de decis√£o e evolu√ß√£o do aprendizado
- [ ] **Otimizadores Avan√ßados**: Adam, RMSprop, Momentum
- [ ] **Regulariza√ß√£o**: Dropout, L1/L2, Batch Normalization
- [ ] **Redes Convolucionais (CNN)**: Implementa√ß√£o b√°sica
- [ ] **Redes Recorrentes (RNN/LSTM)**: Para sequ√™ncias
- [ ] **Autoencoder**: Redu√ß√£o de dimensionalidade
- [ ] **GAN Simples**: Redes advers√°rias generativas
- [ ] **Transfer Learning**: Conceitos b√°sicos
- [ ] **M√©tricas Avan√ßadas**: Precision, Recall, F1-Score

### Melhorias Planejadas
- [ ] **Testes Unit√°rios**: Cobertura completa
- [ ] **Documenta√ß√£o API**: Docstrings detalhadas
- [ ] **Notebooks Jupyter**: Tutoriais interativos
- [ ] **Datasets Reais**: Exemplos com dados do mundo real
- [ ] **Performance**: Otimiza√ß√µes com Numba/Cython

## ü§ù Como Contribuir

### Contribui√ß√µes s√£o muito bem-vindas! üéâ

1. **Fork** este reposit√≥rio
2. **Clone** seu fork: `git clone https://github.com/seu-usuario/neural-networks.git`
3. **Crie uma branch**: `git checkout -b feature/nova-funcionalidade`
4. **Fa√ßa suas altera√ß√µes** e **teste**
5. **Commit**: `git commit -m "Adiciona nova funcionalidade"`
6. **Push**: `git push origin feature/nova-funcionalidade`
7. **Abra um Pull Request**

### Tipos de Contribui√ß√£o
- üêõ **Bug fixes**
- ‚ú® **Novas funcionalidades**
- üìö **Documenta√ß√£o**
- üé® **Melhorias de c√≥digo**
- üß™ **Testes**
- üí° **Exemplos e tutoriais**

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

## üôè Agradecimentos

- **NumPy**: Por tornar computa√ß√£o cient√≠fica acess√≠vel
- **Matplotlib**: Por visualiza√ß√µes incr√≠veis
- **Scikit-learn**: Por datasets e inspira√ß√£o
- **Comunidade Python**: Por todo o conhecimento compartilhado

## üìû Contato

Tem d√∫vidas ou sugest√µes? Abra uma **Issue** ou entre em contato!

---

### ‚≠ê Se este projeto te ajudou, considere dar uma estrela!

**Nota**: Esta implementa√ß√£o foi criada para fins educacionais. Para projetos reais, recomenda-se usar frameworks como TensorFlow, PyTorch ou scikit-learn.